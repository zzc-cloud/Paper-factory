---
name: review-data-domain
description: "Data Analysis & Machine Learning 领域评审认知框架 — 提供特征工程/模型评估/统计显著性领域的核心概念、评估维度和评审方法论"
---

# Review Data Analysis Domain Skill

## 领域认知框架

### 核心研究范式

Data Analysis and Machine Learning 研究基于以下核心范式多

1. **数据驱动范式**多从数据中发现模式，而非依赖先验假设
2. **统计推断范式**多从样本推断总体，量化不确定性
3. **预测建模范式**多构建模型预测未来或未知数据
4. **可重复性范式**多确保分析可重复、结果可靠

### 核心概念与评估维度

#### Feature Engineering

**期望用法**多
- 应说明特征选择和构造方法
- 讨论特征重要性的评估
- 考虑特征与目标变量的关系
- 防止数据泄露（data leakage）

**常见问题**多
- 忽视特征重要性的量化
- 数据泄露（特征中包含目标信息）
- 过度拟合特征
- 缺少特征工程的 justification

**评审要点**多
- 检查特征构造的合理性
- 验证是否有数据泄露
- 评估特征选择的方法
- 确认特征重要性的分析

#### Model Evaluation

**期望用法**多
- 应使用适当的评估指标（准确率、F1、AUC、召回率）
- 根据问题类型选择指标（分类、回归、聚类）
- 使用交叉验证获得可靠估计
- 考虑类别不平衡问题

**常见问题**多
- 仅用准确率评估（尤其在不平衡数据上）
- 忽视类别不平衡的影响
- 没有使用交叉验证
- 测试集泄露（如用于超参数调优）

**评审要点**多
- 检查评估指标的完整性
- 验证交叉验证的正确性
- 评估对不平衡的处理
- 确认数据分割的合理性

#### Cross-Validation

**期望用法**多
- 应说明验证策略（k-fold、分层、时间序列）
- 确保训练/验证/测试集的独立
- 考虑数据的时间特性（如时间序列不能随机分割）
- 报告置信区间或标准差

**常见问题**多
- 数据泄露（如预处理在分割前进行）
- 忽视分布偏移
- 时间序列数据随机分割
- 缺少置信区间

**评审要点**多
- 检查交叉验证策略的正确性
- 验证数据分割的独立性
- 评估是否考虑了时间特性
- 确认不确定性的量化

#### Statistical Significance

**期望用法**多
- 应提供统计显著性检验（t-test, ANOVA, 非参数检验）
- 报告 p 值和置信区间
- 考虑多重检验校正（如 Bonferroni）
- 避免 p-hacking

**常见问题**多
- 缺乏置信区间
- p-hacking（多次试验只报告显著结果）
- 忽略效应大小（只关注 p 值）
- 统计方法使用不当

**评审要点**多
- 检查是否有统计检验
- 验证 p 值的正确使用
- 评估效应大小的报告
- 确认多重检验校正

### 领域评审维度

#### 1. 方法创新性

**评估标准**多
- 是否提出新算法或改进
- 改进的理论或实证支撑
- 与现有方法的区别

**必引经典**多
- Breiman (2001) — Random Forests
- Hastie et al. (2009) — The Elements of Statistical Learning
- Vapnik (1998) — Statistical Learning Theory

#### 2. 实验设计严谨性

**评估标准**多
- 是否有对比基线
- 是否有消融实验
- 数据集选择是否合理
- 实验设置是否可重复

**关键问题**多
- 基线方法是什么？
- 消融实验的结果如何？
- 实验是否可重复？

#### 3. 评估指标全面性

**评估标准**多
- 不能仅用单一指标
- 根据问题类型选择合适指标
- 报告置信区间或标准差
- 考虑实际应用场景

**关键问题**多
- 使用了哪些评估指标？
- 指标选择是否合理？
- 是否有不确定性量化？

#### 4. 数据集选择和代表性

**评估标准**多
- 数据集的代表性
- 数据量是否充足
- 是否有多个数据集验证
- 数据偏见是否考虑

**关键问题**多
- 数据集是否代表实际问题？
- 数据量是否充足？
- 是否有多数据集验证？

#### 5. 可重复性

**评估标准**多
- 代码是否公开
- 数据是否公开（或可用）
- 实验设置描述是否完整
- 超参数是否报告

**关键问题**多
- 代码和数据是否公开？
- 实验设置是否详细描述？
- 结果是否可重复？

#### 6. 与 SOTA 的公平比较

**评估标准**多
- 是否与最新方法对比
- 实验设置是否公平（相同数据、指标）
- 是否讨论了优缺点

**关键问题**多
- 与哪些 SOTA 方法对比？
- 比较是否公平？
- 改进程度如何？

### 常见评审陷阱

#### 陷阱 1多数据泄露（特征或预处理中使用目标信息）

**如何识别**多
- 预处理在数据分割前进行
- 特征包含隐含的目标信息
- 使用未来信息预测过去

**如何避免误判**多
- 检查数据分割的时机
- 确认预处理流程
- 验证特征构造的独立性

#### 陷阱 2多忽视类别不平衡问题

**如何识别**多
- 仅报告准确率
- 数据不平衡但未处理
- 没有使用平衡的评估指标

**如何避免误判**多
- 检查类别分布
- 确认是否使用了合适的指标
- 验证是否采用了平衡策略

#### 陷阱 3多过度拟合（训练集好，测试集差）

**如何识别**多
- 训练集性能远高于测试集
- 没有正则化
- 模型复杂度过高

**如何避免误判**多
- 检查训练/测试性能差距
- 确认正则化策略
- 验证模型复杂度

#### 陷阱 4多Cherry-picking 基线或超参数

**如何识别**多
- 基线方法调优不足
- 自己的方法调优过度
- 只报告最好的结果

**如何避免误判**多
- 检查基线方法的调优
- 确认调优公平性
- 验证结果的完整性

#### 陷阱 5多缺乏统计显著性检验

**如何识别**多
- 没有统计检验
- 只报告平均值
- 改进可能是随机波动

**如何避免误判**多
- 检查是否有统计检验
- 确认置信区间的报告
- 验证效应大小的讨论

### 经典文献对标

#### 必引经典论文

| 论文 | 为什么重要 | 应在何处引用 |
|------|-----------|-------------|
| Breiman (2001) | Random Forests | 相关工作-基线 |
| Hastie et al. (2009) | 统计学习基础 | 引言、方法 |
| Kohavi (1995) | 交叉验证 | 方法-评估 |
| He & Garcia (2009) | 不平衡数据 | 相关工作-不平衡 |
| Dietterich (1998) | 统计检验 | 方法-统计 |

#### SOTA 对比清单

**传统 ML**多SVM, Random Forest, XGBoost, LightGBM

**深度学习**多ResNet, Transformer, BERT, GNN

**领域方法**多根据具体应用领域

**基准数据集**多UCI, Kaggle, ImageNet, 等

### 领域特定评审问题

1. **特征选择**多特征选择是否有理论或实证支持？

2. **消融实验**多是否进行了充分的消融实验验证各组件贡献？

3. **类别不平衡**多如何处理类别不平衡问题？

4. **统计显著性**多是否提供了统计显著性检验和置信区间？

5. **可重复性**多实验是否可重复（代码、数据公开）？

6. **基线对比**多与基线方法的比较是否公平（相同设置、数据）？

7. **过拟合控制**多如何控制过拟合？

8. **数据泄露**多如何避免数据泄露？

9. **模型解释性**多是否有模型解释性分析？

10. **实际部署**多是否考虑了实际部署的挑战？
