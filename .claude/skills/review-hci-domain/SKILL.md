---
name: review-hci-domain
description: "HCI 领域评审认知框架 — 提供可用性/用户体验/用户研究/交互设计领域的核心概念、评估维度和评审方法论"
---

# Review HCI Domain Skill

## 领域认知框架

### 核心研究范式

Human-Computer Interaction (HCI) 研究基于以下核心范式多

1. **以用户为中心范式**多设计围绕用户需求、能力和限制展开
2. **迭代设计范式**多通过设计-评估-改进的循环优化用户体验
3. **情境化范式**多考虑使用场景、社会文化背景和环境因素
4. **可访问性范式**多确保设计对所有用户（包括残障用户）可用

### 核心概念与评估维度

#### Usability

**期望用法**多
- 应遵循可用性原则（易学性、效率、满意度、错误预防）
- 使用标准的可用性评估方法（启发式评估、认知走查）
- 量化可用性指标（任务完成率、完成时间、错误率）
- 考虑不同用户群体的可用性差异

**常见问题**多
- 忽视用户反馈
- 设计不够直观
- 可用性未量化评估
- 只关注某一类用户

**评审要点**多
- 检查可用性原则的应用
- 验证可用性评估的方法
- 评估可用性指标的完整性
- 确认用户群体的多样性

#### User Studies

**期望用法**多
- 应包含用户研究方法（访谈、问卷、观察、实验）
- 确保样本量足够且具有代表性
- 使用适当的分析方法（定性分析、统计分析）
- 考虑伦理问题（知情同意、隐私保护）

**常见问题**多
- 样本量不足
- 用户代表性差（如只用学生）
- 缺少对照组
- 分析方法不当

**评审要点**多
- 检查用户研究方法的合理性
- 验证样本量和代表性
- 评估分析方法的选择
- 确认伦理考虑

#### Interaction Design

**期望用法**多
- 应设计有效的交互流程（导航、操作、反馈）
- 遵循交互设计原则（一致性、反馈、撤销、映射）
- 考虑交互的 Learnability（可学习性）
- 支持不同熟练度的用户

**常见问题**多
- 复杂的操作流程
- 不一致的界面元素
- 缺少反馈机制
- 忽视新手用户

**评审要点**多
- 检查交互流程的设计
- 验证一致性原则
- 评估反馈机制
- 确认对不同用户的支持

#### User-Centered Design (UCD)

**期望用法**多
- 应遵循以用户为中心的设计流程
- 在设计早期引入用户参与
- 迭代设计并持续评估
- 平衡用户需求和技术约束

**常见问题**多
- 缺乏迭代设计
- 设计师主导而非用户主导
- 用户参与不足
- 忽视用户反馈

**评审要点**多
- 检查 UCD 流程的完整性
- 验证用户参与的时机和方式
- 评估迭代次数和改进
- 确认用户反馈的整合

#### Accessibility

**期望用法**多
- 应考虑可访问性设计（WCAG 标准）
- 支持辅助技术（屏幕阅读器、放大镜）
- 考虑不同能力用户的需求
- 进行可访问性测试

**常见问题**多
- 忽视残障用户
- 缺乏辅助技术支持
- 不符合 WCAG 标准
- 没有可访问性测试

**评审要点**多
- 检查 WCAG 合规性
- 验证辅助技术支持
- 评估可访问性测试
- 确认包容性设计

### 领域评审维度

#### 1. 用户体验质量

**评估标准**多
- 易学性
- 效率
- 满意度
- 错误率

**必引经典**多
- Norman (1988) — The Design of Everyday Things
- Shneiderman (1998) — Designing the User Interface
- Nielsen (1993) — Usability Engineering

#### 2. 界面设计有效性

**评估标准**多
- 视觉层次
- 信息架构
- 导航设计
- 反馈机制

**关键问题**多
- 界面布局是否合理？
- 导航是否直观？
- 反馈是否及时？

#### 3. 用户研究严谨性

**评估标准**多
- 方法选择的合理性
- 样本代表性
- 分析方法的适当性
- 伦理考虑

**关键问题**多
- 用户研究方法是否合适？
- 样本是否代表目标用户？
- 分析方法是否严谨？

#### 4. 与 HCI 最佳实践对比

**评估标准**多
- 是否遵循设计原则
- 与标准模式的对比
- 创新的合理性

**关键问题**多
- 遵循了哪些设计原则？
- 与标准模式有何不同？
- 创新是否合理？

#### 5. 实际用户评估

**评估标准**多
- 是否有真实用户测试
- 测试场景的真实性
- 用户反馈的整合

**关键问题**多
- 是否有用户测试？
- 测试场景是否真实？
- 用户反馈如何整合？

#### 6. 可访问性考虑

**评估标准**多
- WCAG 合规性
- 辅助技术支持
- 包容性设计

**关键问题**多
- 是否符合 WCAG 标准？
- 支持哪些辅助技术？
- 如何考虑包容性？

### 常见评审陷阱

#### 陷阱 1多设计师中心而非用户中心

**如何识别**多
- 设计决策缺少用户依据
- 没有用户研究
- 设计师假设用户需求

**如何避免误判**多
- 检查用户研究的参与
- 确认设计决策的依据
- 验证用户反馈的整合

#### 陷阱 2多忽视边缘用户群体

**如何识别**多
- 只考虑典型用户
- 忽视老年人、残障人士等
- 缺少多样性考虑

**如何避免误判**多
- 检查用户群体的多样性
- 确认边缘用户的考虑
- 验证包容性设计

#### 陷阱 3多过度依赖美学而非可用性

**如何识别**多
- 强调视觉效果但可用性差
- 缺少可用性评估
- 美学优先于功能

**如何避免误判**多
- 检查可用性评估
- 确认功能与美学的平衡
- 验证用户任务的支持

#### 陷阱 4多缺乏用户测试

**如何识别**多
- 没有用户测试环节
- 只有专家评审
- 缺少真实场景测试

**如何避免误判**多
- 检查用户测试的设计
- 确认测试场景的真实性
- 验证测试结果的整合

#### 陷阱 5多不一致的交互模式

**如何识别**多
- 相同功能不同交互方式
- 界面元素风格不统一
- 交互逻辑不一致

**如何避免误判**多
- 检查一致性的维护
- 确认设计系统的使用
- 验证交互模式的一致性

### 经典文献对标

#### 必引经典论文

| 论文 | 为什么重要 | 应在何处引用 |
|------|-----------|-------------|
| Norman (1988) | 设计心理学基础 | 引言、原则 |
| Shneiderman (1998) | 界面设计经典 | 相关工作-设计 |
| Nielsen (1993) | 可用性工程 | 方法-评估 |
| Sharp et al. (2007) | 交互设计 | 相关工作-方法 |
| ISO 9241-11 | 可用性标准 | 方法-评估 |

#### SOTA 对比清单

**设计原则**多Nielsen's Heuristics, Shneiderman's 8 Golden Rules

**评估方法**多启发式评估、认知走查、用户测试、眼动追踪

**可访问性**多WCAG 2.1, Section 508

**研究方法**多访谈、问卷、观察、实验、案例研究

### 领域特定评审问题

1. **用户研究方法**多用户研究方法如何选择和执行？

2. **界面设计响应**多界面设计如何响应用户需求？

3. **可用性测试影响**多可用性测试结果如何影响设计决策？

4. **可访问性标准**多设计是否符合可访问性标准？

5. **用户体验改进**多用户体验如何持续改进？

6. **不同用户群体**多如何考虑不同用户群体的需求？

7. **错误处理**多错误处理和反馈如何设计？

8. **学习曲线**多学习曲线如何评估和优化？

9. **跨文化考虑**多是否考虑了跨文化差异？

10. **长期使用**多是否考虑了长期使用的体验？
