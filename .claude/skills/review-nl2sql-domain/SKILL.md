---
name: review-nl2sql-domain
description: "NL2SQL 领域评审认知框架 — 提供 Schema Linking/SQL 生成/泛化领域的核心概念、评估维度和评审方法论"
---

# Review NL2SQL Domain Skill

## 领域认知框架

### 核心研究范式

Natural Language to SQL (NL2SQL/Text2SQL) 研究基于以下核心范式多

1. **语义映射范式**多将自然语言映射到结构化查询语言（SQL）
2. **Schema Linking 范式**多建立自然语言短语与数据库 schema 的链接
3. **神经符号范式**多结合神经网络的语言理解能力和符号化的 SQL 语法
4. **交互式范式**多通过多轮交互澄清查询意图

### 核心概念与评估维度

#### Schema Linking

**期望用法**多
- 应正确定义如何将自然语言短语映射到表、列、关系
- 处理同义词和歧义（如 "name" 可能指多列）
- 考虑 schema 的层次结构（表-列-关系）
- 支持 schema 的泛化能力

**常见问题**多
- 忽视值域模糊性（如 "high" 如何映射）
- 缺少歧义处理机制
- 过度依赖精确匹配
- 未考虑跨表的相似列名

**评审要点**多
- 检查 schema linking 的方法是否清晰
- 验证歧义处理策略是否充分
- 评估对同义词和变体的处理能力
- 确认是否考虑了 schema 演化

#### SQL Synthesis

**期望用法**多
- 应生成符合语法的 SQL 查询
- 支持嵌套查询、复杂连接、聚合函数
- 考虑 SQL 方言差异（MySQL, PostgreSQL, SQL Server）
- 处理查询执行的边界情况

**常见问题**多
- SQL 注入风险（如果应用于实际系统）
- 不支持嵌套查询或多跳连接
- 生成低效查询（如笛卡尔积）
- 错误使用聚合函数和 GROUP BY

**评审要点**多
- 检查生成 SQL 的语法正确性
- 验证是否支持复杂查询构造
- 评估 SQL 方言兼容性
- 确认查询效率的考量

#### Execution Feedback

**期望用法**多
- 应利用查询执行结果改进后续查询
- 处理执行错误并给出反馈
- 学习用户偏好（如查询风格）
- 支持查询结果的可视化

**常见问题**多
- 忽视错误反馈的利用
- 缺少从执行失败中学习的机制
- 未考虑查询优化建议
- 没有处理空结果集

**评审要点**多
- 检查是否有执行反馈循环
- 验证错误处理策略
- 评估学习机制的有效性
- 确认用户体验设计

#### Generalization (泛化能力)

**期望用法**多
- 应讨论跨数据库/跨领域的泛化能力
- 处理未见过的 schema
- 迁移学习策略
- Zero-shot / Few-shot 学习

**常见问题**多
- 仅在单一数据集评估
- 忽视 schema 差异的影响
- 缺少跨领域验证
- 未讨论泛化瓶颈

**评审要点**多
- 检查泛化能力的评估设置
- 验证跨数据库/跨领域实验
- 评估迁移学习策略
- 确认泛化限制的讨论

### 领域评审维度

#### 1. Schema Linking 准确性

**评估标准**多
- 是否正确识别表、列、关系
- 对同义词和歧义的处理能力
- 跨表相似列名的区分能力

**必引经典**多
- Popescu et al. (2003) — 基于规则的 NL2SQL
- Li et al. (2020) — IRNet（隐式 schema linking）

#### 2. SQL 查询正确性和执行效率

**评估标准**多
- SQL 语法正确率
- 查询执行效率（执行时间、资源占用）
- 对复杂查询的支持（嵌套、多跳连接）

**关键问题**多
- SQL 语法正确率如何？
- 是否支持嵌套查询和复杂连接？
- 查询效率如何评估？

#### 3. 对模糊和复杂查询的处理能力

**评估标准**多
- 对模糊/歧义查询的处理策略
- 多轮交互的澄清机制
- 复杂查询的分解策略

**关键问题**多
- 如何处理模糊查询（如 "high revenue"）？
- 是否支持多轮交互澄清？
- 复杂查询如何分解？

#### 4. 跨数据库泛化能力

**评估标准**多
- 是否在多个数据库上评估
- 对未见过的 schema 的适应能力
- 迁移学习的效果

**关键问题**多
- 是否评估了跨数据库泛化？
- 如何处理未见过的 schema？
- 迁移学习策略如何？

#### 5. 与最新方法的比较

**评估标准**多
- 是否与 Spider、WikiSQL 等基准对比
- 是否与基于 LLM 的方法对比
- 实验设置是否公平

**SOTA 对比清单**多
- 传统方法多Seq2SQL, TypeSQL, IRNet
- LLM 方法多GPT-4 based, DAIL-SQL, C3
- 基准多Spider, WikiSQL, CoSQL, SParC

#### 6. 用户研究或真实场景评估

**评估标准**多
- 是否有用户研究
- 在真实场景的部署和评估
- 用户满意度和效率评估

**关键问题**多
- 是否有用户研究？
- 真实场景的评估结果如何？
- 用户满意度如何？

### 常见评审陷阱

#### 陷阱 1多仅在单一、干净的 schema 上评估

**如何识别**多
- 只使用一个数据库
- Schema 设计非常规整，无歧义
- 缺少跨数据库实验

**如何避免误判**多
- 检查是否使用了多个数据集
- 确认 schema 的多样性
- 验证是否有跨数据库实验

#### 陷阱 2多忽视 SQL 注入等安全问题

**如何识别**多
- 生成的 SQL 直接用于实际系统
- 没有输入验证和清理
- 未讨论安全风险

**如何避免误判**多
- 检查是否讨论了安全问题
- 确认是否有输入验证
- 验证是否限制了 SQL 操作范围

#### 陷阱 3多对复杂查询支持不足

**如何识别**多
- 只评估简单查询（单表查询）
- 不支持嵌套、多跳连接
- 实验集中复杂查询比例低

**如何避免误判**多
- 检查查询复杂度分布
- 确认是否评估了复杂查询
- 验证对嵌套查询的支持

#### 陷阱 4多缺少与最新 LLM-based 方法对比

**如何识别**多
- 相关工作只包含传统方法
- 未引用 GPT-4、DAIL-SQL 等最新工作
- 实验设置不与 SOTA 对比

**如何避免误判**多
- 检查相关工作是否包含最新方法
- 确认是否有与 LLM 方法的对比
- 验证实验的公平性

#### 陷阱 5多忽视跨数据库泛化挑战

**如何识别**多
- 只在 Spider 单一基准上评估
- 没有讨论 schema 差异的影响
- 缺少跨领域实验

**如何避免误判**多
- 检查是否有跨数据库实验
- 确认是否讨论了泛化瓶颈
- 验证迁移学习策略

### 经典文献对标

#### 必引经典论文

| 论文 | 为什么重要 | 应在何处引用 |
|------|-----------|-------------|
| Androutsopoulos et al. (1990s) | NLIDB 早期工作 | 相关工作-历史 |
| Popescu et al. (2003) | 结构化映射 | 相关工作-schema linking |
| Zhong et al. (2017) Seq2SQL | 神经 NL2SQL 开端 | 相关工作-神经方法 |
| Yu et al. (2018) | Schema linking | 方法-核心模块 |
| Wang et al. (2020) RAT-SQL | 关系感知 | 相关工作-SOTA |
| Scholak et al. (2021) | Spider 基准 | 实验-对比 |

#### SOTA 对比清单

**传统神经方法**多Seq2SQL, TypeSQL, IRNet, X-SQL, RAT-SQL

**LLM 方法**多DAIL-SQL, C3, GPT-4 based, CoderX

**基准数据集**多Spider, WikiSQL, CoSQL, SParC, KaggleDBQA

**评估指标**多Exact Match, Execution Accuracy, Efficiency

### 领域特定评审问题

1. **Schema Linking 歧义处理**多Schema Linking 模块如何处理歧义（如同名列）？

2. **复杂查询支持**多是否支持多跳查询和复杂嵌套？

3. **泛化能力**多如何处理超出训练 schema 的查询？

4. **基准对比**多是否与 Spider、WikiSQL 等基准对比？

5. **跨数据库泛化**多是否评估了跨数据库泛化能力？

6. **用户交互**多是否支持多轮交互澄清？

7. **错误恢复**多查询失败时如何反馈和恢复？

8. **LLM 对比**多是否与最新的 LLM-based 方法对比？

9. **查询优化**多是否考虑了查询执行效率？

10. **真实部署**多是否有真实场景的部署和评估？
