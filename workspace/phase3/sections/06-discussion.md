## 6. Discussion

The experimental results presented in Section 5 provide comprehensive empirical validation of the Cognitive Hub architecture. We now interpret these findings in the broader context of enterprise-scale natural language data querying, discuss the limitations of our approach, and examine the generalizability of the architectural principles to other domains and applications.

### 6.1 Key Findings and Implications

**Ontology as Cognitive Hub Outperforms Passive Knowledge Structures.** The comparison between Smart Query (TLA@1 = 0.82) and baseline systems reveals a clear hierarchy of effectiveness. Direct LLM reasoning without structured knowledge (B0: 0.48) fails catastrophically at enterprise scale, confirming that LLMs cannot navigate 35,000+ tables through parametric knowledge alone. RAG with vector retrieval (B1: 0.61) provides modest improvement but remains inadequate — semantic similarity captures surface-level lexical overlap but misses the hierarchical business logic and data lineage relationships that govern enterprise data architectures. The substantial gap between B1 and Smart Query (Δ = +0.21, p < 0.001) validates our central thesis: a domain ontology must function as an active cognitive layer, not merely a passive retrieval index.

The architectural distinction is fundamental. RAG systems treat knowledge as an unstructured document collection where retrieval is the primary operation. The Cognitive Hub architecture treats knowledge as a structured cognitive resource with multiple navigation strategies, each exploiting different structural properties. The three-layer ontology separation (indicators, data assets, terms) enables orthogonal exploration strategies that RAG's flat vector space cannot support. This finding has implications beyond Smart Query: it suggests that enterprise AI systems requiring deep domain reasoning should invest in structured knowledge architectures rather than relying solely on embedding-based retrieval.

**Serial Execution with Context Inheritance Outperforms Parallel Approaches.** The comparison between Smart Query and B4 (Parallel Execution: TLA@1 = 0.76, Δ = +0.06, p < 0.05) provides direct empirical validation of the semantic cumulative effect. The entropy analysis strengthens this conclusion: Smart Query achieves 14% lower final entropy than B4 (H₃ = 4.08 vs 4.73 bits, p < 0.01), with the advantage widening for Complex (19%) and Adversarial (23%) queries. This validates the information-theoretic prediction that serial execution with context inheritance enables focused search that parallel execution cannot achieve.

The ablation study reinforces this finding: removing context inheritance (A1) causes the largest performance degradation (Δ = -0.13, p < 0.001), with effects concentrated on Complex and Adversarial queries where progressive refinement is most valuable. The failure case analysis (Section 5.5) reveals that context degradation — where the LLM fails to extract relevant information from conversation history — is the primary failure mode, highlighting that implicit context inheritance depends critically on LLM contextual understanding quality.

This finding challenges the prevailing assumption in LLM-based multi-agent systems that parallel execution is preferable for efficiency. While parallel execution reduces latency, it sacrifices the cumulative semantic enrichment that serial execution provides. The optimal design choice depends on the task structure: for tasks requiring multi-perspective evidence fusion where perspectives can inform each other (as in enterprise data querying), serial execution with context inheritance is superior despite the latency overhead.

**Evidence Pack Fusion Provides Calibrated Confidence.** The positive correlation between Evidence Consensus Score (ECS) and accuracy (Spearman ρ = 0.67, p < 0.001) demonstrates that inter-strategy agreement is a reliable confidence indicator. Queries with three-strategy consensus (ECS = 1.0) achieve 94% accuracy, while single-strategy recommendations (ECS = 0.33) achieve only 58% accuracy. This graded confidence mechanism enables the system to communicate uncertainty to users — a critical capability for enterprise deployment where incorrect recommendations can have significant consequences.

The ablation study (A2: remove evidence fusion) shows that relying on a single strategy's recommendation degrades TLA@1 by 0.11 (p < 0.01), with the largest effects on Complex queries. This validates the design principle that multi-strategy fusion is more robust than any single strategy. The cross-validation adjudication mechanism is more sophisticated than simple voting: it operates on structured evidence packs containing candidate tables, confidence scores, and reasoning traces, enabling nuanced reconciliation that binary voting cannot achieve.

This finding has implications for LLM-based decision systems more broadly. Rather than treating LLM outputs as binary predictions, systems should collect structured evidence from multiple perspectives and use inter-perspective agreement as a confidence signal. This approach parallels ensemble methods in machine learning but operates at the evidence level rather than the prediction level.

**Lineage-Driven JOIN Discovery Outperforms Semantic Similarity.** The comparison between Smart Query's lineage-driven JOIN discovery (JA-F1 = 0.81) and schema-based column name matching (A4 ablation: JA-F1 = 0.58, Δ = +0.23, p < 0.01) validates a key design principle: for relational operations, structural facts about data flow are more reliable than semantic similarity guesses. Case Study 2 (Section 5.5) illustrates this concretely: lineage correctly identifies "customer_info" as the JOIN target based on actual ETL relationships, while column name matching incorrectly proposes "loan_application" based on coincidental name overlap.

This finding challenges the prevailing trend toward embedding-based approaches for all data integration tasks. While semantic similarity is valuable for discovering conceptual relationships, it is insufficient for inferring operational relationships like JOINs that depend on actual data flow. Enterprise data warehouses already contain rich lineage metadata from ETL pipelines; Smart Query demonstrates that this structural information should be leveraged directly rather than approximated through semantic similarity.

The broader implication is that hybrid approaches combining structural facts with semantic reasoning outperform purely semantic approaches. The dual retrieval mechanism (Innovation 6) embodies this principle: Strategy 2 combines convergent path navigation (structural) with hybrid search (semantic), achieving higher Field Coverage Rate (FCR = 0.88) than either approach alone. This suggests a general design pattern for enterprise AI systems: exploit structural metadata where available, use semantic reasoning to fill gaps.

**Isolated Table Filtering Provides Zero-Maintenance Quality Assurance.** The ablation study (A3) shows that isolated table filtering provides a modest but statistically significant precision improvement (Δ = -0.07 in TLA@1 when removed, p < 0.05). Case Study 3 demonstrates the mechanism in action: deprecated tables with high name similarity to the query are correctly excluded based on graph-theoretic connectivity analysis (in-degree + out-degree = 0). This validates the design principle that data quality can be inferred from graph topology without manual curation.

The practical value lies in zero-maintenance operation. Traditional data quality approaches require manual metadata tagging or periodic audits to identify deprecated tables. Smart Query's graph-theoretic filter operates automatically on the existing lineage graph, requiring no additional metadata or human intervention. This is particularly valuable in enterprise environments where data assets evolve continuously and manual curation cannot keep pace.

The cognitive architecture analogy is apt: just as ACT-R's base-level activation decay causes unused knowledge to become inaccessible [Anderson et al., 2004], Smart Query's isolated table filtering causes unused data assets to be automatically excluded. This suggests a broader principle: cognitive architectures for enterprise systems should incorporate temporal validity mechanisms that reflect actual usage patterns.

### 6.2 Limitations

**Domain Specificity and Ontology Construction Cost.** Smart Query's ontology is banking-specific, constructed through a 21-step ETL pipeline that extracts metadata from source systems, computes lineage relationships, and builds the three-layer knowledge graph. This construction process required substantial domain expertise and engineering effort. While the ontology enables powerful reasoning capabilities, it represents a significant upfront investment that may not be feasible for all domains or organizations.

The ontology construction challenge is not unique to Smart Query — it is inherent to any knowledge-intensive system. Traditional OBDA systems face the same challenge of creating formal ontologies and mappings [Ontop, DL-Lite]. Smart Query's advantage is that once the ontology is constructed, it supports flexible natural language querying without requiring rigid formal query languages. Nevertheless, the construction cost remains a barrier to adoption.

A related limitation is domain transferability. The three-layer structure (indicators, data assets, terms) reflects banking domain characteristics and may not generalize directly to other domains. Healthcare, for example, might require different layer structures to capture clinical concepts, patient records, and medical terminology. While the Cognitive Hub architecture is domain-agnostic, the specific ontology design must be tailored to each domain's knowledge structure.

**Lack of Formal Correctness Guarantees.** Unlike traditional OBDA systems that provide formal correctness guarantees through query rewriting [Ontop, DL-Lite], Smart Query's LLM-based reasoning is probabilistic. The system can produce incorrect recommendations when context inheritance fails, keyword ambiguity causes semantic drift, or cross-validation adjudication selects the wrong consensus. The failure case analysis (Section 5.5) reveals that context degradation is the primary failure mode, occurring when the LLM fails to extract relevant information from conversation history.

This limitation is fundamental to LLM-based systems. While formal OBDA systems guarantee correctness within their expressiveness limits, they cannot handle queries outside those limits. Smart Query trades formal guarantees for flexibility — it can handle ambiguous, incomplete, or underspecified queries that would fail in formal systems. The graded confidence mechanism (ECS) partially mitigates this limitation by communicating uncertainty, but it does not eliminate the possibility of confident incorrect recommendations.

Future work could explore hybrid approaches that combine formal reasoning where possible with LLM-based reasoning for ambiguous cases. For example, when a query maps unambiguously to a single indicator with a single associated table, the system could bypass LLM reasoning and return the formal mapping directly. This would provide correctness guarantees for the subset of queries that admit formal solutions while retaining flexibility for complex cases.

**Serial Execution Latency Overhead.** Serial execution with three strategies introduces latency overhead compared to parallel approaches. In our deployment, the average query resolution time is approximately 15-20 seconds (Strategy 1: 5-7s, Strategy 2: 6-8s, Strategy 3: 4-5s), compared to 8-10 seconds for parallel execution (B4). For interactive applications where sub-second response times are expected, this latency may be unacceptable.

The latency-accuracy tradeoff is inherent to the serial execution design. The semantic cumulative effect requires that later strategies observe earlier strategies' findings, which necessitates sequential execution. Parallel execution eliminates this dependency but sacrifices the cumulative semantic enrichment that drives Smart Query's accuracy advantage.

Potential mitigations include: (1) adaptive strategy selection that executes only the strategies likely to contribute new information based on query characteristics, (2) early termination when a strategy achieves high-confidence consensus, (3) speculative parallel execution with post-hoc context integration, or (4) caching of common query patterns to bypass strategy execution entirely. These optimizations could reduce latency while preserving most of the accuracy benefits.

**Dependence on LLM Contextual Understanding Quality.** The implicit context inheritance mechanism depends critically on the LLM's ability to extract relevant information from conversation history. The failure case analysis reveals that context degradation — where the LLM fails to leverage previous strategies' findings — is the primary failure mode. This limitation is specific to LLM-based implementations and may improve as LLM architectures advance, but it represents a current constraint.

The ablation study (A1: remove context inheritance) quantifies this dependence: removing context inheritance degrades TLA@1 by 0.13, with larger effects on Complex and Adversarial queries. This suggests that approximately 16% of Smart Query's performance (0.13 / 0.82) depends on successful context inheritance. When context inheritance fails, the system degrades to the independent agents baseline (B3: 0.73), which still outperforms single-strategy and RAG baselines but loses the cumulative semantic enrichment advantage.

Improving context inheritance robustness requires better prompt engineering to explicitly instruct later strategies to extract and leverage previous findings, potentially with structured context extraction mechanisms that parse previous evidence packs into explicit constraints for subsequent strategies. This would make context inheritance more explicit and less dependent on implicit LLM inference.

**Evaluation on Standard Benchmarks.** Smart Query has not been evaluated on standard NL2SQL benchmarks like Spider or Bird. This is a deliberate choice reflecting different problem scopes: Spider contains 200 small databases (average <10 tables each) designed for cross-database generalization, while Smart Query addresses single-domain enterprise scale (35,287 tables). The skills required for these tasks differ: Spider emphasizes SQL syntax generation and cross-domain generalization, while Smart Query emphasizes schema navigation and domain knowledge integration.

Nevertheless, the lack of benchmark evaluation limits comparability with existing NL2SQL systems. Future work could adapt Smart Query's architecture to benchmark settings by constructing lightweight ontologies for Spider/Bird databases, enabling direct comparison with systems like MAC-SQL, DIN-SQL, and CHESS. This would clarify whether the Cognitive Hub architecture provides advantages beyond enterprise-specific deployment.

### 6.3 Generalizability

**Domain-Agnostic Architecture with Domain-Specific Instantiation.** While Smart Query's ontology is banking-specific, the Cognitive Hub architecture is domain-agnostic. The core principles — separating declarative memory (ontology) from procedural memory (Skills), coordinating through an LLM pattern-matching engine, using multiple orthogonal navigation strategies with serial execution and implicit context inheritance — apply to any domain with complex structured knowledge.

Healthcare provides a natural application domain. A healthcare ontology could separate clinical concepts (diagnoses, procedures, medications), patient data assets (EHR tables, lab results, imaging), and medical terminology (SNOMED CT, ICD codes). Three strategies could navigate these layers: a Clinical Concept Expert exploring diagnosis hierarchies, a Data Asset Navigator following EHR schema structures, and a Terminology Analyst searching standardized medical terms. The same evidence pack fusion and lineage-driven relationship discovery mechanisms would apply.

Manufacturing offers another application domain. A manufacturing ontology could separate production indicators (yield, defect rates, cycle times), equipment and process data (sensor tables, maintenance logs), and engineering terminology (part specifications, quality standards). The three-strategy pattern would map naturally: an Indicator Expert for production metrics, a Process Navigator for equipment hierarchies, and a Terminology Analyst for engineering standards.

The key requirement for domain transfer is that the domain exhibits hierarchical knowledge structure with multiple orthogonal dimensions. Domains with flat, unstructured knowledge may not benefit from the Cognitive Hub architecture. The three-layer separation (business concepts, data assets, terminology) appears to be a common pattern across enterprise domains, suggesting broad applicability.

**Multi-Strategy Serial Execution as a General Pattern.** The multi-strategy serial execution pattern with implicit context inheritance generalizes beyond data querying to any task requiring multi-perspective reasoning where perspectives can inform each other. Potential applications include:

- **Multi-Source Intelligence Analysis**: Different strategies could analyze signals intelligence, human intelligence, and open-source intelligence, with later strategies focusing on regions identified by earlier strategies.

- **Medical Diagnosis**: Different strategies could explore symptom patterns, lab results, and imaging findings, with later strategies prioritizing tests suggested by earlier findings.

- **Legal Research**: Different strategies could search case law, statutes, and secondary sources, with later strategies focusing on jurisdictions and time periods identified by earlier strategies.

- **Scientific Literature Review**: Different strategies could explore citation networks, keyword searches, and author networks, with later strategies prioritizing papers in clusters identified by earlier strategies.

The common pattern is that multiple independent perspectives provide complementary evidence, and serial execution with context inheritance enables later perspectives to focus on high-value regions identified by earlier perspectives. The semantic cumulative effect formalization (Section 4.1) provides theoretical grounding for when this pattern is beneficial: when perspectives explore orthogonal knowledge dimensions and context inheritance enables focused search.

**Multi-Scenario Ontology Extension.** The current Smart Query ontology focuses on the Data Query scenario — mapping business questions to physical tables and fields. The broader enterprise data management landscape includes additional scenarios: Data Development (designing new data assets), Data Governance (ensuring data quality and compliance), and Data Lineage Analysis (understanding data flow and impact). Each scenario requires different navigation strategies over the same underlying ontology.

A multi-scenario extension could define scenario-specific Skills that navigate the ontology for different purposes. Data Development Skills might navigate from business requirements to candidate table designs, exploring similar existing tables and reusable data standards. Data Governance Skills might navigate from compliance requirements to affected tables, exploring data lineage to identify downstream impacts. This extension would transform the ontology from a query-specific resource into a general-purpose cognitive hub for all data management activities.

The multi-scenario extension validates the Cognitive Hub architecture's flexibility. The same declarative memory (ontology) supports multiple procedural memories (scenario-specific Skills), with the LLM pattern-matching engine coordinating between them. This separation of concerns — knowledge representation (ontology) from knowledge utilization (Skills) — is a fundamental principle of cognitive architectures [ACT-R, SOAR] that Smart Query demonstrates in an enterprise AI context.

---

The discussion reveals that Smart Query's success stems from principled architectural choices grounded in cognitive science and information theory. The Cognitive Hub architecture, multi-strategy serial execution with implicit context inheritance, and evidence pack fusion with cross-validation adjudication are not ad-hoc engineering decisions but theoretically motivated designs with empirical validation. The limitations — domain specificity, lack of formal guarantees, latency overhead, and LLM dependence — are inherent tradeoffs rather than implementation flaws, and they suggest clear directions for future work. The generalizability analysis demonstrates that the architectural principles extend beyond banking data querying to other domains and scenarios requiring multi-perspective reasoning over structured knowledge. In Section 7, we conclude with a summary of contributions and an outlook on future research directions.
